{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli = load_dataset(\"glue\", \"mnli\")\n",
    "hans = load_dataset(\"hans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_mnli(dataset, remove_neutral=True):\n",
    "    if remove_neutral:\n",
    "        # neutral class has label 1\n",
    "        dataset = dataset.filter(lambda example: example[\"label\"] != 1)\n",
    "\n",
    "    # change labels of contradiction examples from 2 to 1\n",
    "    def change_label(example):\n",
    "        # convert labels 2 into labels 1. this merges the neutral and contradiction class\n",
    "        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "        return example\n",
    "        \n",
    "    # change labels\n",
    "    dataset = dataset.map(change_label)\n",
    "\n",
    "    # change features to reflect the new labels\n",
    "    features = dataset[\"train\"].features.copy()\n",
    "    features[\"label\"] = ClassLabel(num_classes=2, names=['entailment', 'contradiction'], id=None)\n",
    "    dataset = dataset.cast(features)  # overwrite old features\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli = binarize_mnli(mnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "seed = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yes' token id: 10932\n",
      "'no' token id: 2362\n"
     ]
    }
   ],
   "source": [
    "yes_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "no_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "print(f\"'yes' token id: {yes_id}\")\n",
    "print(f\"'no' token id: {no_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffccdcf323543d6ba3fb2f2341c074a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/261802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734056e36ef9466887d985fabcdd5c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a007a820a24a958d6a44fe2417b5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8d177ebad54e139f159bf408ac1f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0050fbbb9d2145d18a2b7c96f3143b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_context = \"Given the premise and hypothesis: reply yes if the premise entails the hypothesis, or no otherwise\"\n",
    "\n",
    "def generate_mnli_prompts(examples):\n",
    "    # teacher model receives task context + premise + hypothesis\n",
    "    examples[\"teacher_prompt\"] = f\"{task_context}\\nPremise: {examples['premise']}\\nHypothesis: {examples['hypothesis']}\"\n",
    "    # student model only receives premise + hypothesis\n",
    "    examples[\"student_prompt\"] = f\"Premise: {examples['premise']}\\nHypothesis: {examples['hypothesis']}\"\n",
    "    return examples\n",
    "\n",
    "mnli = mnli.map(generate_mnli_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20\n",
    "np.random.seed(seed)\n",
    "selected_idx = np.random.choice(1000, train_size)\n",
    "\n",
    "mnli_train = mnli[\"train\"].select(selected_idx)\n",
    "labels_train = torch.tensor(mnli_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ad6922e7164d60b2bca8b02f1e641e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_teacher(data):\n",
    "    tokens = tokenizer(data[\"teacher_prompt\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_teacher_mnli_train = mnli_train.map(tokenize_teacher, batched=True)\n",
    "tokenized_teacher_mnli_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenized_teacher_mnli_train[\"input_ids\"]\n",
    "attention_mask = tokenized_teacher_mnli_train[\"attention_mask\"]\n",
    "teacher_model.eval()\n",
    "\n",
    "outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "logits = outputs.logits\n",
    "    \n",
    "teacher_logits = logits[:, -1, [yes_id, no_id]]\n",
    "teacher_pred = logits[:, -1, [yes_id, no_id]].argmax(dim=-1)\n",
    "teacher_acc = (teacher_pred == labels_train).float().mean().item()\n",
    "teacher_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_distillation_loss(labels, teacher_logits, student_logits, alpha=0.5):\n",
    "    with torch.no_grad():\n",
    "        teacher_logprob = torch.nn.functional.softmax(teacher_logits, dim=-1)\n",
    "    student_prob = torch.nn.functional.log_softmax(student_logits, dim=-1)\n",
    "    kl_loss = torch.nn.functional.kl_div(student_prob, teacher_logprob, reduction=\"batchmean\")\n",
    "    ce_loss = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "    beta = 1 - alpha\n",
    "    cd_loss = alpha * kl_loss + beta * ce_loss\n",
    "    return cd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5891a38519644df4bd821454f233aa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_student(data):\n",
    "    tokens = tokenizer(data[\"student_prompt\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_student_mnli_train = mnli_train.map(tokenize_student, batched=True)\n",
    "tokenized_student_mnli_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, inputs, labels):\n",
    "\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "    target_logits = logits[:, -1, [yes_id, no_id]]\n",
    "    pred = target_logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "    ce_loss = torch.nn.functional.cross_entropy(target_logits, labels).item()\n",
    "\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    return ce_loss, acc\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8ab6b632c34e15a0ae0d4e2f0a3464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_size = 20\n",
    "np.random.seed(seed + 1)\n",
    "selected_idx_eval = np.random.choice(1000, eval_size)\n",
    "\n",
    "mnli_eval = mnli[\"train\"].select(selected_idx_eval)\n",
    "labels_eval = torch.tensor(mnli_eval[\"label\"])\n",
    "tokenized_student_mnli_eval = mnli_eval.map(tokenize_student, batched=True)\n",
    "tokenized_student_mnli_eval.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_model(student_model, lr, epochs, train_tokenized, train_labels, validation_tokenized, validation_labels, teacher_logits, target_tokens):\n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        input_ids = train_tokenized[\"input_ids\"]\n",
    "        attention_mask = train_tokenized[\"attention_mask\"]\n",
    "\n",
    "        outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        student_target_logits = logits[:, -1, target_tokens]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cd_loss = context_distillation_loss(train_labels, teacher_logits, student_target_logits)\n",
    "        cd_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = student_target_logits.argmax(dim=-1)\n",
    "        train_acc = (pred == train_labels).float().mean().item()\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(student_model, validation_tokenized, validation_labels)\n",
    "\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"\\tTraining Loss: {cd_loss.item():.4f}\\t\\tTraining Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"\\tValidation Loss: {val_loss:.4f}\\t\\tValidation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    train_loss, train_acc = evaluate_model(student_model, train_tokenized, train_labels)\n",
    "    val_loss, val_acc = evaluate_model(student_model, validation_tokenized, validation_labels)\n",
    "    print(\"Final model\")\n",
    "    print(f\"\\tTraining Loss: {train_loss:.4f}\\t\\tTraining Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"\\tValidation Loss: {val_loss:.4f}\\t\\tValidation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]\n",
      "\tTraining Loss: 0.3568\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.4873\t\tValidation Accuracy: 0.7500\n",
      "Epoch [2/30]\n",
      "\tTraining Loss: 0.3647\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 1.0290\t\tValidation Accuracy: 0.2500\n",
      "Epoch [3/30]\n",
      "\tTraining Loss: 0.5094\t\tTraining Accuracy: 0.7500\n",
      "\tValidation Loss: 0.5738\t\tValidation Accuracy: 0.8500\n",
      "Epoch [4/30]\n",
      "\tTraining Loss: 0.2948\t\tTraining Accuracy: 0.9000\n",
      "\tValidation Loss: 0.4685\t\tValidation Accuracy: 0.7500\n",
      "Epoch [5/30]\n",
      "\tTraining Loss: 0.3497\t\tTraining Accuracy: 0.5000\n",
      "\tValidation Loss: 0.4726\t\tValidation Accuracy: 0.7500\n",
      "Epoch [6/30]\n",
      "\tTraining Loss: 0.3339\t\tTraining Accuracy: 0.5000\n",
      "\tValidation Loss: 0.5181\t\tValidation Accuracy: 0.8500\n",
      "Epoch [7/30]\n",
      "\tTraining Loss: 0.2722\t\tTraining Accuracy: 0.7500\n",
      "\tValidation Loss: 0.6347\t\tValidation Accuracy: 0.7500\n",
      "Epoch [8/30]\n",
      "\tTraining Loss: 0.2701\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.7193\t\tValidation Accuracy: 0.3500\n",
      "Epoch [9/30]\n",
      "\tTraining Loss: 0.2929\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.6888\t\tValidation Accuracy: 0.5000\n",
      "Epoch [10/30]\n",
      "\tTraining Loss: 0.2775\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5991\t\tValidation Accuracy: 0.8000\n",
      "Epoch [11/30]\n",
      "\tTraining Loss: 0.2520\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5270\t\tValidation Accuracy: 0.8000\n",
      "Epoch [12/30]\n",
      "\tTraining Loss: 0.2471\t\tTraining Accuracy: 0.9000\n",
      "\tValidation Loss: 0.4927\t\tValidation Accuracy: 0.8000\n",
      "Epoch [13/30]\n",
      "\tTraining Loss: 0.2549\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.4833\t\tValidation Accuracy: 0.7500\n",
      "Epoch [14/30]\n",
      "\tTraining Loss: 0.2558\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.4904\t\tValidation Accuracy: 0.8500\n",
      "Epoch [15/30]\n",
      "\tTraining Loss: 0.2463\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5144\t\tValidation Accuracy: 0.8000\n",
      "Epoch [16/30]\n",
      "\tTraining Loss: 0.2385\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5485\t\tValidation Accuracy: 0.8000\n",
      "Epoch [17/30]\n",
      "\tTraining Loss: 0.2396\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5741\t\tValidation Accuracy: 0.7500\n",
      "Epoch [18/30]\n",
      "\tTraining Loss: 0.2442\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5746\t\tValidation Accuracy: 0.7500\n",
      "Epoch [19/30]\n",
      "\tTraining Loss: 0.2433\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5531\t\tValidation Accuracy: 0.8000\n",
      "Epoch [20/30]\n",
      "\tTraining Loss: 0.2375\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5265\t\tValidation Accuracy: 0.8000\n",
      "Epoch [21/30]\n",
      "\tTraining Loss: 0.2341\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5085\t\tValidation Accuracy: 0.8000\n",
      "Epoch [22/30]\n",
      "\tTraining Loss: 0.2362\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5028\t\tValidation Accuracy: 0.8500\n",
      "Epoch [23/30]\n",
      "\tTraining Loss: 0.2392\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5076\t\tValidation Accuracy: 0.8500\n",
      "Epoch [24/30]\n",
      "\tTraining Loss: 0.2385\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5216\t\tValidation Accuracy: 0.8000\n",
      "Epoch [25/30]\n",
      "\tTraining Loss: 0.2354\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5423\t\tValidation Accuracy: 0.8000\n",
      "Epoch [26/30]\n",
      "\tTraining Loss: 0.2341\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5624\t\tValidation Accuracy: 0.8000\n",
      "Epoch [27/30]\n",
      "\tTraining Loss: 0.2357\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5718\t\tValidation Accuracy: 0.8000\n",
      "Epoch [28/30]\n",
      "\tTraining Loss: 0.2369\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5658\t\tValidation Accuracy: 0.8000\n",
      "Epoch [29/30]\n",
      "\tTraining Loss: 0.2355\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5496\t\tValidation Accuracy: 0.8000\n",
      "Epoch [30/30]\n",
      "\tTraining Loss: 0.2334\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5327\t\tValidation Accuracy: 0.8000\n",
      "Final model\n",
      "\tTraining Loss: 0.3034\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.5327\t\tValidation Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "student_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-5)\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    input_ids = tokenized_student_mnli_train[\"input_ids\"]\n",
    "    attention_mask = tokenized_student_mnli_train[\"attention_mask\"]\n",
    "\n",
    "    outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    student_logits = logits[:, -1, [yes_id, no_id]]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cd_loss = context_distillation_loss(labels_train, teacher_logits, student_logits)\n",
    "    cd_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pred = student_logits.argmax(dim=-1)\n",
    "    train_acc = (pred == labels_train).float().mean().item()\n",
    "    \n",
    "    val_loss, val_acc = evaluate_model(student_model, tokenized_student_mnli_eval, labels_eval)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    print(f\"\\tTraining Loss: {cd_loss.item():.4f}\\t\\tTraining Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"\\tValidation Loss: {val_loss:.4f}\\t\\tValidation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "train_loss, train_acc = evaluate_model(student_model, tokenized_student_mnli_train, labels_train)\n",
    "val_loss, val_acc = evaluate_model(student_model, tokenized_student_mnli_eval, labels_eval)\n",
    "print(\"Final model\")\n",
    "print(f\"\\tTraining Loss: {train_loss:.4f}\\t\\tTraining Accuracy: {train_acc:.4f}\")\n",
    "print(f\"\\tValidation Loss: {val_loss:.4f}\\t\\tValidation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-domain Accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "indomain_size = 100\n",
    "np.random.seed(seed)\n",
    "indomain_idx = np.random.choice(mnli[\"validation_mismatched\"].num_rows, indomain_size)\n",
    "indomain = mnli[\"validation_mismatched\"].select(indomain_idx)\n",
    "indomain_labels = torch.tensor(indomain[\"label\"])\n",
    "\n",
    "tokenized_indomain = indomain.map(tokenize_student, batched=True)\n",
    "tokenized_indomain.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "loss, acc = evaluate_model(student_model, tokenized_indomain, indomain_labels)\n",
    "print(f\"In-domain Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b04b608fea74bfb81635868f435d6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f16b66820bf427185145ef84733336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess HANS dataset \n",
    "\n",
    "\n",
    "# add student prompt\n",
    "def generate_hans_prompts(examples):\n",
    "    # student model only receives premise + hypothesis\n",
    "    examples[\"student_prompt\"] = f\"Premise: {examples['premise']}\\nHypothesis: {examples['hypothesis']}\"\n",
    "    return examples\n",
    "\n",
    "hans = hans.map(generate_hans_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e717eedd46dc4e228b602a549284c07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-domain Accuracy: 0.4800\n"
     ]
    }
   ],
   "source": [
    "outdomain_size = 100\n",
    "np.random.seed(seed)\n",
    "outdomain_idx = np.random.choice(hans[\"validation\"].num_rows, outdomain_size)\n",
    "outdomain = hans[\"validation\"].select(outdomain_idx)\n",
    "outdomain_labels = torch.tensor(outdomain[\"label\"])\n",
    "\n",
    "tokenized_outdomain = outdomain.map(tokenize_student, batched=True)\n",
    "tokenized_outdomain.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "loss, acc = evaluate_model(student_model, tokenized_outdomain, outdomain_labels)\n",
    "print(f\"Out-domain Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model and compute in-domain and out-domain accuracies\n",
    "def compute_model_performance(model_name, train_dataset, validation_dataset, indomain_dataset, outdomain_dataset, \n",
    "                              epochs, lr):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # compare yes/no token logit/probability for context distillation\n",
    "    yes_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "    no_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "    target_token_ids = [yes_id, no_id]\n",
    "    \n",
    "    # Pre-computing teacher logits to train student\n",
    "    tokenized_teacher_train = train_dataset.map(tokenize_teacher, batched=True)\n",
    "    tokenized_teacher_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    teacher_input_ids = tokenized_teacher_train[\"input_ids\"]\n",
    "    teacher_attention_mask = tokenized_teacher_train[\"attention_mask\"]\n",
    "    \n",
    "    # disable gradients\n",
    "    teacher_model.eval()\n",
    "    teacher_logits = teacher_model(input_ids=teacher_input_ids, attention_mask=teacher_attention_mask).logits\n",
    "        \n",
    "    # extract yes/no logits\n",
    "    teacher_target_logits = teacher_logits[:, -1, target_token_ids]\n",
    "    teacher_pred = teacher_target_logits.argmax(dim=-1)\n",
    "    teacher_acc = (teacher_pred == labels_train).float().mean().item()\n",
    "    print(f\"Teacher Training Accuracy: {teacher_acc}\")\n",
    "\n",
    "    # train student model\n",
    "    tokenized_student_train = train_dataset.map(tokenize_student, batched=True)\n",
    "    tokenized_student_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    tokenized_student_val = validation_dataset.map(tokenize_student, batched=True)\n",
    "    tokenized_student_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    print(\"Training student model:\")\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    train_labels = torch.tensor(train_dataset[\"label\"])\n",
    "    validation_labels = torch.tensor(validation_dataset[\"label\"])\n",
    "    train_student_model(student_model, lr, epochs, tokenized_student_train, train_labels, tokenized_student_val, validation_labels, teacher_target_logits, target_token_ids)\n",
    "\n",
    "    # compute in-domain accuracy\n",
    "    tokenized_indomain = indomain_dataset.map(tokenize_student, batched=True)\n",
    "    tokenized_indomain.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    indomain_labels = torch.tensor(indomain_dataset[\"label\"])\n",
    "    id_loss, id_acc = evaluate_model(student_model, tokenized_indomain, indomain_labels)\n",
    "    print(f\"In-domain Accuracy: {id_acc:.4f}\")\n",
    "\n",
    "    # compute out-domain accuracy\n",
    "    tokenized_outdomain = outdomain_dataset.map(tokenize_student, batched=True)\n",
    "    tokenized_outdomain.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    outdomain_labels = torch.tensor(outdomain_dataset[\"label\"])\n",
    "    od_loss, od_acc = evaluate_model(student_model, tokenized_outdomain, outdomain_labels)\n",
    "    print(f\"Out-domain Accuracy: {od_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"In-domain Accuracy\": id_acc,\n",
    "        \"Out-domain Accuracy\": od_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli = load_dataset(\"glue\", \"mnli\")\n",
    "hans = load_dataset(\"hans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing MNLI dataset\n",
    "\n",
    "\n",
    "# convert MNLI into binary classification\n",
    "def binarize_mnli(dataset, remove_neutral=True):\n",
    "    if remove_neutral:\n",
    "        # neutral class has label 1\n",
    "        dataset = dataset.filter(lambda example: example[\"label\"] != 1)\n",
    "\n",
    "    # change labels of contradiction examples from 2 to 1\n",
    "    def change_label(example):\n",
    "        # convert labels 2 into labels 1. this merges the neutral and contradiction class\n",
    "        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "        return example\n",
    "        \n",
    "    # change labels\n",
    "    dataset = dataset.map(change_label)\n",
    "\n",
    "    # change features to reflect the new labels\n",
    "    features = dataset[\"train\"].features.copy()\n",
    "    features[\"label\"] = ClassLabel(num_classes=2, names=['entailment', 'contradiction'], id=None)\n",
    "    dataset = dataset.cast(features)  # overwrite old features\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# add teacher and student prompts\n",
    "task_context = \"Given the premise and hypothesis: reply yes if the premise entails the hypothesis, or no otherwise\"\n",
    "\n",
    "def generate_mnli_prompts(examples):\n",
    "    # teacher model receives task context + premise + hypothesis\n",
    "    examples[\"teacher_prompt\"] = f\"{task_context}\\nPremise: {examples['premise']}\\nHypothesis: {examples['hypothesis']}\"\n",
    "    # student model only receives premise + hypothesis\n",
    "    examples[\"student_prompt\"] = f\"Premise: {examples['premise']}\\nHypothesis: {examples['hypothesis']}\"\n",
    "    return examples\n",
    "\n",
    "\n",
    "mnli = binarize_mnli(mnli)\n",
    "mnli = mnli.map(generate_mnli_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train, validation, indomain, outdomain datasets\n",
    "\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# train dataset (MNLI train)\n",
    "train_size = 20     # few shot examples\n",
    "train_idx = np.random.choice(mnli[\"train\"].num_rows, train_size)\n",
    "train_dataset = mnli[\"train\"].select(train_idx)\n",
    "\n",
    "# validation dataset (MNLI validation matched)\n",
    "val_size = 20\n",
    "val_idx = np.random.choice(mnli[\"validation_matched\"].num_rows, val_size)\n",
    "validation_dataset = mnli[\"validation_matched\"].select(val_idx)\n",
    "\n",
    "# indomain dataset (MNLI validation mismatched)\n",
    "indomain_size = 100\n",
    "indomain_idx = np.random.choice(mnli[\"validation_mismatched\"].num_rows, indomain_size)\n",
    "indomain_dataset = mnli[\"validation_mismatched\"].select(indomain_idx)\n",
    "\n",
    "# outdomain (HANS validation)\n",
    "outdomain_size = 100\n",
    "outdomain_idx = np.random.choice(hans[\"validation\"].num_rows, outdomain_size)\n",
    "outdomain_dataset = hans[\"validation\"].select(outdomain_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: facebook/opt-125m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022eb209cf6e4afeae8edfd1e3f94759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Training Accuracy: 0.44999998807907104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4b62f4c3844ecea9511a6cdc95704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f452feb87fe84f71945b9074ff43331d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training student model:\n",
      "Epoch [1/30]\n",
      "\tTraining Loss: 0.4500\t\tTraining Accuracy: 0.4000\n",
      "\tValidation Loss: 0.8251\t\tValidation Accuracy: 0.4500\n",
      "Epoch [2/30]\n",
      "\tTraining Loss: 0.3942\t\tTraining Accuracy: 0.5000\n",
      "\tValidation Loss: 0.8014\t\tValidation Accuracy: 0.5000\n",
      "Epoch [3/30]\n",
      "\tTraining Loss: 0.4022\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.7920\t\tValidation Accuracy: 0.5000\n",
      "Epoch [4/30]\n",
      "\tTraining Loss: 0.3891\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.7870\t\tValidation Accuracy: 0.5000\n",
      "Epoch [5/30]\n",
      "\tTraining Loss: 0.3770\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.7785\t\tValidation Accuracy: 0.4500\n",
      "Epoch [6/30]\n",
      "\tTraining Loss: 0.3749\t\tTraining Accuracy: 0.5000\n",
      "\tValidation Loss: 0.7634\t\tValidation Accuracy: 0.5000\n",
      "Epoch [7/30]\n",
      "\tTraining Loss: 0.3672\t\tTraining Accuracy: 0.5000\n",
      "\tValidation Loss: 0.7445\t\tValidation Accuracy: 0.5000\n",
      "Epoch [8/30]\n",
      "\tTraining Loss: 0.3583\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.7257\t\tValidation Accuracy: 0.5000\n",
      "Epoch [9/30]\n",
      "\tTraining Loss: 0.3531\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.7114\t\tValidation Accuracy: 0.5000\n",
      "Epoch [10/30]\n",
      "\tTraining Loss: 0.3482\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.7021\t\tValidation Accuracy: 0.5000\n",
      "Epoch [11/30]\n",
      "\tTraining Loss: 0.3416\t\tTraining Accuracy: 0.5500\n",
      "\tValidation Loss: 0.6971\t\tValidation Accuracy: 0.5000\n",
      "Epoch [12/30]\n",
      "\tTraining Loss: 0.3346\t\tTraining Accuracy: 0.6500\n",
      "\tValidation Loss: 0.6958\t\tValidation Accuracy: 0.5000\n",
      "Epoch [13/30]\n",
      "\tTraining Loss: 0.3280\t\tTraining Accuracy: 0.7000\n",
      "\tValidation Loss: 0.6973\t\tValidation Accuracy: 0.5000\n",
      "Epoch [14/30]\n",
      "\tTraining Loss: 0.3201\t\tTraining Accuracy: 0.8000\n",
      "\tValidation Loss: 0.7009\t\tValidation Accuracy: 0.5000\n",
      "Epoch [15/30]\n",
      "\tTraining Loss: 0.3099\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.7067\t\tValidation Accuracy: 0.5000\n",
      "Epoch [16/30]\n",
      "\tTraining Loss: 0.2989\t\tTraining Accuracy: 0.9000\n",
      "\tValidation Loss: 0.7154\t\tValidation Accuracy: 0.5000\n",
      "Epoch [17/30]\n",
      "\tTraining Loss: 0.2891\t\tTraining Accuracy: 0.9000\n",
      "\tValidation Loss: 0.7274\t\tValidation Accuracy: 0.5000\n",
      "Epoch [18/30]\n",
      "\tTraining Loss: 0.2810\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.7424\t\tValidation Accuracy: 0.5000\n",
      "Epoch [19/30]\n",
      "\tTraining Loss: 0.2741\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.7587\t\tValidation Accuracy: 0.5000\n",
      "Epoch [20/30]\n",
      "\tTraining Loss: 0.2682\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.7729\t\tValidation Accuracy: 0.5000\n",
      "Epoch [21/30]\n",
      "\tTraining Loss: 0.2635\t\tTraining Accuracy: 0.8500\n",
      "\tValidation Loss: 0.7818\t\tValidation Accuracy: 0.5000\n",
      "Epoch [22/30]\n",
      "\tTraining Loss: 0.2593\t\tTraining Accuracy: 0.9000\n",
      "\tValidation Loss: 0.7836\t\tValidation Accuracy: 0.5000\n",
      "Epoch [23/30]\n",
      "\tTraining Loss: 0.2550\t\tTraining Accuracy: 0.9000\n",
      "\tValidation Loss: 0.7791\t\tValidation Accuracy: 0.5000\n",
      "Epoch [24/30]\n",
      "\tTraining Loss: 0.2509\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.7710\t\tValidation Accuracy: 0.5000\n",
      "Epoch [25/30]\n",
      "\tTraining Loss: 0.2481\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.7625\t\tValidation Accuracy: 0.5000\n",
      "Epoch [26/30]\n",
      "\tTraining Loss: 0.2469\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.7562\t\tValidation Accuracy: 0.5000\n",
      "Epoch [27/30]\n",
      "\tTraining Loss: 0.2468\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.7533\t\tValidation Accuracy: 0.5000\n",
      "Epoch [28/30]\n",
      "\tTraining Loss: 0.2474\t\tTraining Accuracy: 1.0000\n",
      "\tValidation Loss: 0.7542\t\tValidation Accuracy: 0.5000\n",
      "Epoch [29/30]\n",
      "\tTraining Loss: 0.2484\t\tTraining Accuracy: 0.9500\n",
      "\tValidation Loss: 0.7583\t\tValidation Accuracy: 0.5000\n",
      "Epoch [30/30]\n",
      "\tTraining Loss: 0.2490\t\tTraining Accuracy: 0.9500\n",
      "\tValidation Loss: 0.7646\t\tValidation Accuracy: 0.5000\n",
      "Final model\n",
      "\tTraining Loss: 0.2663\t\tTraining Accuracy: 0.9500\n",
      "\tValidation Loss: 0.7646\t\tValidation Accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b986d06db2474e888e9faa10fa621bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-domain Accuracy: 0.5300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdb0294fdb74deaad2d5c643712dcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-domain Accuracy: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'In-domain Accuracy': 0.5299999713897705,\n",
       " 'Out-domain Accuracy': 0.550000011920929}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "epochs = 30\n",
    "lr = 1e-5\n",
    "\n",
    "compute_model_performance(model_name=model_name, train_dataset=train_dataset, validation_dataset=validation_dataset, \n",
    "                          indomain_dataset=indomain_dataset, outdomain_dataset=outdomain_dataset, epochs=epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
